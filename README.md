# Building-an-end-to-end-data-pipeline-using-Azure-Databricks-Delta-Lake
how to develop an end-to-end data pipeline using Delta Lake which is an open-source storage layer that provides ACID transactions and metadata handling. Also you will learn how data moves from bronze to gold container, how to make an incremental load, create external tables for data analysis and orchestrate your pipeline. 

Technologies used: PySpark, ADSLS, Azure Databricks, Azure Data Factory and Power BI,App registrations,Key vault


Requirements:

1.Azure Account (Free Trial Subscription)

2.Basic knowledge of Azure

3.Basic knowledge of Databricks

4.Basic knowledge of Python and PySpark

![image](https://github.com/user-attachments/assets/5c838af6-fa44-4e77-9843-759bf513252e)


Process:

1.Create Resource Group

2.create a databricks workspace,and set up cluster.

3.create a storage account and then the layers we are going to need for our pipeline (bronze, silver, gold) according to Delta Lake Architecture.

4.make sure to check the hierarchial namespace whether ticked or not.

5.open Azure Active Directory, click on App registrations on the left panel and then on New Registration.

6.once created, save the application Id, object Id and Directory Id on a notepad, we will need them, later

7.It’s very important to copy the secret value and keep it in a notepad, because once you come out of the screen, you won’t be able to get it again.

![image](https://github.com/user-attachments/assets/076afdf5-b748-4fb1-8e9e-0768addb2c7d)



8.Go to your storage account, on the left panel click on Access Control (IAM) and then on Add Role Assignment.
Select the “Storage Blob Data Contributor” role, then click on next or go to Members tab

![image](https://github.com/user-attachments/assets/771f5603-e541-413e-b427-121fb359c036)

9.On Members Tab, click on “+ Select members” and find your databricks-service-app and select it. Finally click on Review + assign

![image](https://github.com/user-attachments/assets/6fd6f8ae-c08b-4a01-bba6-b79ef892938c)

![image](https://github.com/user-attachments/assets/41d616c9-95b5-4fd4-9fd5-59597fa97fa2)

10.After your Key-Vault has been created, go to Secrets inside Objects section and click on Generate/Import

11.create 3 secreats wich we have saved earlier

![image](https://github.com/user-attachments/assets/b78b3d7b-1fe8-4104-a7bd-ccd0959346d2)

12.Go to the properties of your Key-vault and copy the Vault URI and Resource ID values in a notepad

![image](https://github.com/user-attachments/assets/99177556-ffec-4858-b07f-713df1947526)

13.Link Databricks Secret Scopes with Azure Key-vault,Launch your Databricks Workspace, and go to your portal page. Once you are there you need to add “secrets/createScope” after the “#” symbol in the URL and click enter.

![image](https://github.com/user-attachments/assets/4016624a-8e1b-44b6-a8f8-6b4ae31e28d9)

14.On the new screen displayed, set an scope-name, set manage principal to All Users and also fill the DNS Name (Vault URI) and Resource Id with the corresponding values from your Azure Key-Vault (Step 3) and click on Create.

![image](https://github.com/user-attachments/assets/2381c12b-30cf-4c4b-9084-fda2cc1adffb)

15.Mount azure containers into databricks,Create a workspace with the name “moun-adls-storage”  and inside it a folder called set-up

![image](https://github.com/user-attachments/assets/49e3d286-7072-4bd1-92f3-42d63d626be8)

16.Open your notebook and start your cluster.(check with adlsmount_notebook file)

Use Case Explanation
We will be working with transactional data referred to loan transactions and customers from GeekBankPE (a famous bank around the world).

You have two requirements from different areas of the bank.

1.The Marketing area needs to have updated customer data to be able to contact them and make offers.

2.The Finance area requires to have daily loan transactions complemented with customer drivers to be able to analyze them and improve the revenue.

To comply with the request, we are going to perform incremental loads and also using techniques like upsert.

![image](https://github.com/user-attachments/assets/93516ada-1655-49fd-8111-938a7ed58f29)


Data Ingestion and Transformation:

About dataset:

1.Customer: This data needs to be treated with an UPSERT technique because it only send new and modified records daily.
- customerId: unique identifier
- firstName: name
- lastName: last name
- phone: home phone number
- email: email
- gender: Male or Female
- address: place where the customer lives
- is_active: flag that indicates if the client is with us

2.Customer Drivers: This data is generated daily from the RiskModeling area. The data that is sent is an snapshot of the day. This data will be loaded incrementally.
- date: date that the data was generated by RiskModeling area
- customerId: unique identifier of the customer
- monthly_salary: monthly salary in USD
- health_score: score - how important is the customer for the bank
- current_debt: current debt that the customer has with our bank
- category: segment of the customer

3.Loan Transactions: Data correspond to the transactions performed in a specific date. This data will be loaded incrementally.
- date: date of the transaction
- customerId: unique identifier of the customer
- paymentPeriod: term of the loan
- loanAmount: amount requested by the customer
- currencyType: currency of the loan (USD, EUR)
- evaluationChannel: channel by which the loan was sold
- interest_rate: rate of the loan

1.Upload data to bronze layer either using adf pipeline or using Azure Storage Explorer

2.download : https://azure.microsoft.com/en-us/products/storage/storage-explorer/#overview

3.Once you have set up your Storage Explorer, you just need to drag and drop the folders in it



4.The execution order of our files is the following:

![image](https://github.com/user-attachments/assets/bc5045d2-57c7-45bb-ab3a-f9f4dcba231f)

5.On your databricks workspace create a folder called "ingestion" and inside it the following notebooks:
- customer.py
- customerDriver.py
- loanTransaction.py
  
Also in your databricks workspace create a folder called "includes" and inside it the following notebooks:
- common_functions.py
- configurations.py

![image](https://github.com/user-attachments/assets/386b2f39-6c8e-453c-9ee4-88a824c0fff5)

![image](https://github.com/user-attachments/assets/7955384f-b719-40f6-917c-941a036ca7f7)

6.Complete all databricks notebooks

7.Testing our files,A very easy way to test our files is by creating another python file wherever you want

![image](https://github.com/user-attachments/assets/42adeeae-84bc-416b-b67c-d565a6a96614)

8.Data Enrichment:we are going to perform some joins with our tables, aggregations, we will use upsert technique and finally query our tables

On your databricks workspace create a folder named enrichment & create 2 new python files named:
- customer.py
- loantTrx.py

On your databricks workspace create a set-up folder create a new python file named:
- database.py

9.Run all notebook and test it by uncommenting the lines which we have commented earlier.


10.Querying database tables:Inside your utilities folder, create a SQL_queries Notebook

![alt text](Images\image-2.png)


Set up Data Factory:

1.create a data factory,On Git configuration tab mark Configure Git later, because is not necessary

2.Generate Access Token for Data Factory,

Go to your databricks workspace, select settings > User Settings.

![alt text](Images/image-3.png)

Select Generate a new token, set the lifetime days and click on Generate.

![alt text](Images/image.png)

Copy the token to a notepad because we will need it later

![alt text](Images/image-1.png)

3.Create a pipeline in adf,

create a pipeline and give 
Settings:
— Concurrency = 1
Parameters:
— Create “p_processing_date” param of type string.

4.in the pipeline ,search for notebook

5.We need to create a databricks linked service.Select your activity go to Azure Databricks tab and click on New

![image](https://github.com/user-attachments/assets/dacd698a-00d9-4e01-9581-5ef8637ab758)

![image](https://github.com/user-attachments/assets/d9ca3f3c-5eaa-4953-9f22-890b64195dcc)

![image](https://github.com/user-attachments/assets/166bc6bf-a1c3-46be-94fd-97a69045c4bb)

Go the Base parameters section and add one parameter named “p_file_date” (Note this param should have the same name, we use on our notebooks on databricks). Click on Add dynamic content.

![image](https://github.com/user-attachments/assets/79cc89cf-d584-498a-a2bf-492c297eb4d7)

Select your parameter and use the @formatDateTime(date, str_format) to cast it from date to string and click on OK.
@formatDateTime(pipeline().parameters.p_processing_date, ‘yyyy-MM-dd’)

![image](https://github.com/user-attachments/assets/1ca781cd-9350-4113-867e-d129fa0928dd)

6.follow the same procedure and configure

![image](https://github.com/user-attachments/assets/f8d768d8-8ffa-4b3b-9439-f06f51d9bfd3)

7.create a tumbling window trigger with start and end end data  and set max concurrency=1

8.After clicking OK, on the new screen we need to send a value to our pipeline parameter, set “@trigger().outputs.windowEndTime” and click OK.
@trigger().outputs.windowEndTime End of the window associated with the trigger run

![image](https://github.com/user-attachments/assets/eecc0fce-9468-4f27-8cfa-7d4532af11ef)

9.Before Publishing our pipeline, delete the files from your silver and gold container































